{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 인식 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Class: 치즈 Confidence Score: 92 %\n",
      "인식된 음식: 치즈. 맞습니까? (y/n)\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Class: 양파 Confidence Score: 100 %\n",
      "인식된 음식: 양파. 맞습니까? (y/n)\n",
      "추가할 식재료가 있습니까? (y/n)\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Class: 치즈 Confidence Score: 100 %\n",
      "인식된 음식: 치즈. 맞습니까? (y/n)\n",
      "추가할 식재료가 있습니까? (y/n)\n",
      "예측된 식재료: 양파 치즈\n",
      "레시피: 양파치즈빵 만들기\n",
      "레시피: 어니언 치즈 스콘 만들기｜효썸\n",
      "레시피: 시금치 양파 치즈 스크램블에그 \n",
      "레시피: 치아바타샌드위치 - 양파치즈샌드위치 \n",
      "레시피: 지지고 볶아 달달한 양파가 치즈사이로 쏙! 양파치즈샌드위치\n",
      "추천 레시피:\n",
      "Food Name: 양파치즈빵 만들기\n",
      "Ingredients: 양파, 치즈\n",
      "Recipe:\n",
      "1. [반죽] 양파를 사용할 때는 채 썰어서 사용해주세요.\n",
      "2. 볼에 버터를 제외한 모든 재료를 넣고 손이나 반죽길 5분간 반죽해주세요.\n",
      "3. 재료가 뭉쳐지기 시작하면 버터를 넣고 20분간 매끄럽게 반죽해주세요.\n",
      "4. 따뜻한 곳(40℃)에 50분간 1차 발효시켜주세요.\n",
      "5. 1차 발효 후.\n",
      "6. 반죽을 4등분으로 나눠 둥글려주세요.\n",
      "7. 랩을 덮어 15분간 중간발효 시켜주세요.\n",
      "8. 0.5cm 두께가 되도록 길게 밀어주세요.\n",
      "9. 돌돌 말아주세요.\n",
      "10. 꼼꼼하게 마무리까지 했으면, 반죽의 윗면을 납작하게 눌러주세요.\n",
      "11. 따뜻한 곳(40℃)에 30~40분간 2차 발효시켜주세요.\n",
      "12. 2차 발효 후.\n",
      "13. 가운데에 길게 칼집을 내주세요.(깊이 0.5~0.7cm 정도)\n",
      "14. 채 썬 양파를 칼집을 기준으로 적당히 올려주세요.\n",
      "15. 마요네즈를 짤주머니 또는 지퍼백에 담아\n",
      "16. 가늘게 뿌려주세요.\n",
      "17. 피자치즈로 양파를 덮고, 파슬리가루를 약간 뿌려 마무리해주세요.\n",
      "18. 170℃에서 15분간 노릇하게 구워주세요.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.layers import DepthwiseConv2D\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def food_info(names):\n",
    "    query = '+'.join(names)\n",
    "    url = f\"https://www.10000recipe.com/recipe/list.html?q={query}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    else:\n",
    "        print(\"HTTP 응답 오류:\", response.status_code)\n",
    "        return\n",
    "\n",
    "    food_list = soup.find_all(attrs={'class': 'common_sp_link'})\n",
    "    if not food_list:\n",
    "        print(\"식재료에 맞는 레시피가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    recipes = []\n",
    "\n",
    "    for food in food_list[:5]:\n",
    "        food_id = food['href'].split('/')[-1]\n",
    "        new_url = f'https://www.10000recipe.com/recipe/{food_id}'\n",
    "        new_response = requests.get(new_url)\n",
    "        if new_response.status_code == 200:\n",
    "            html = new_response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "        else:\n",
    "            print(\"HTTP 응답 오류:\", new_response.status_code)\n",
    "            continue\n",
    "\n",
    "        food_info = soup.find(attrs={'type': 'application/ld+json'})\n",
    "        if food_info is not None:\n",
    "            result = json.loads(food_info.text)\n",
    "            ingredients = result.get('recipeIngredient', [])\n",
    "            recipe = [step['text'] for step in result.get('recipeInstructions', [])]\n",
    "            for i in range(len(recipe)):\n",
    "                recipe[i] = f'{i + 1}. ' + recipe[i]\n",
    "\n",
    "            view_count_element = soup.find(attrs={'class': 'view_count'})\n",
    "            view_count = int(view_count_element.text.replace(',', '')) if view_count_element else 0\n",
    "\n",
    "            print(f\"레시피: {soup.title.string}\")\n",
    "\n",
    "            recipes.append({\n",
    "                'title': soup.title.string,\n",
    "                'ingredients': ingredients,\n",
    "                'recipe': recipe,\n",
    "                'view_count': view_count\n",
    "            })\n",
    "\n",
    "    if recipes:\n",
    "        best_recipe = max(recipes, key=lambda x: x['view_count'])\n",
    "\n",
    "        res = {\n",
    "            'name': best_recipe['title'],\n",
    "            'ingredients': ', '.join(best_recipe['ingredients']),\n",
    "            'recipe': best_recipe['recipe']\n",
    "        }\n",
    "        return res\n",
    "    else:\n",
    "        print(\"적합한 레시피를 찾지 못했습니다.\")\n",
    "        return\n",
    "\n",
    "# 모델과 라벨 로드\n",
    "model = load_model(\"C:/Users/youngjin/Desktop/keras_model.h5\", \n",
    "                   custom_objects={'DepthwiseConv2D': DepthwiseConv2D}, \n",
    "                   compile=False)\n",
    "\n",
    "class_names = open(\"C:/Users/youngjin/Desktop/converted_keras/labels.txt\", \"r\",encoding=(\"utf_8\")).readlines()\n",
    "\n",
    "# 웹캠 초기화\n",
    "camera = cv2.VideoCapture(0)\n",
    "is_recognizing = True\n",
    "predicted_classes = []\n",
    "\n",
    "while True:\n",
    "    ret, image = camera.read()\n",
    "    if not ret:\n",
    "        print(\"웹캠 이미지 캡처 실패\")\n",
    "        break\n",
    "\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    cv2.imshow(\"Webcam Image\", resized_image)\n",
    "\n",
    "    if is_recognizing:\n",
    "        image = np.asarray(resized_image, dtype=np.float32)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image / 255.0\n",
    "\n",
    "        prediction = model.predict(image)\n",
    "        index = np.argmax(prediction)\n",
    "        class_name = class_names[index].strip()\n",
    "        confidence_score = prediction[0][index]\n",
    "\n",
    "        print(\"Class:\", class_name[2:], \"Confidence Score:\", str(np.round(confidence_score * 100))[:-2], \"%\")\n",
    "\n",
    "        if confidence_score > 0.8:  # 낮은 신뢰도 예측을 피하기 위한 임계값 설정\n",
    "            print(f\"인식된 음식: {class_name[2:]}. 맞습니까? (y/n)\")\n",
    "            while True:\n",
    "                keyboard_input = cv2.waitKey(1)\n",
    "                if keyboard_input == ord('y'):\n",
    "                    predicted_classes.append(class_name[2:])\n",
    "                    print(\"추가할 식재료가 있습니까? (y/n)\")\n",
    "                    while True:\n",
    "                        additional_input = cv2.waitKey(1)\n",
    "                        if additional_input == ord('y'):\n",
    "                            is_recognizing = True\n",
    "                            break\n",
    "                        elif additional_input == ord('n'):\n",
    "                            is_recognizing = False\n",
    "                            break\n",
    "                        elif additional_input == 27:  # ESC 키를 누르면 종료\n",
    "                            is_recognizing = False\n",
    "                            break\n",
    "                    break\n",
    "                elif keyboard_input == ord('n'):\n",
    "                    is_recognizing = True  # 인식을 계속 진행\n",
    "                    break\n",
    "                elif keyboard_input == 27:  # ESC 키를 누르면 종료\n",
    "                    is_recognizing = False\n",
    "                    break\n",
    "\n",
    "    if not is_recognizing:\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if predicted_classes:\n",
    "    print(\"예측된 식재료:\", \" \".join(predicted_classes))\n",
    "    food_info_result = food_info(predicted_classes)\n",
    "    if food_info_result:\n",
    "        print(\"추천 레시피:\")\n",
    "        print(\"Food Name:\", food_info_result[\"name\"])\n",
    "        print(\"Ingredients:\", \", \".join(predicted_classes))\n",
    "        print(\"Recipe:\")\n",
    "        for step in food_info_result['recipe']:\n",
    "            print(step)\n",
    "else:\n",
    "    print(\"식재료를 인식하지 못했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n",
      "findfont: Font family 'NanumGothic' not found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# NanumGothic-Regular 폰트 파일 경로 설정\n",
    "font_path = 'C:/Users/youngjin/Desktop/Nanum_Gothic/NanumGothic-Regular.ttf'\n",
    "if not os.path.exists(font_path):\n",
    "    raise FileNotFoundError(f\"Font file not found: {font_path}\")\n",
    "\n",
    "# 폰트 등록 및 설정\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = font_prop.get_name()\n",
    "\n",
    "# 디렉토리 경로 설정\n",
    "base_dir = \"C:/Users/youngjin/Desktop/design/\"\n",
    "\n",
    "# 각 범주에 있는 이미지 파일 수를 세는 함수\n",
    "def count_images_in_directories(base_dir):\n",
    "    categories = []\n",
    "    image_counts = []\n",
    "\n",
    "    for category in os.listdir(base_dir):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            image_files = [f for f in os.listdir(category_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "            categories.append(category)\n",
    "            image_counts.append(len(image_files))\n",
    "\n",
    "    return categories, image_counts\n",
    "\n",
    "# 데이터 수집\n",
    "categories, image_counts = count_images_in_directories(base_dir)\n",
    "\n",
    "# 막대 그래프 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, image_counts, color='skyblue')\n",
    "plt.xlabel('카테고리')\n",
    "plt.ylabel('이미지 수')\n",
    "plt.title('카테고리별 이미지 수')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.h5의 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model('C:/Users/youngjin/Desktop/CNN_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15164 images belonging to 8 classes.\n",
      "{'감자': 0, '김치': 1, '달걀': 2, '마늘': 3, '빵': 4, '양파': 5, '치즈': 6, '파': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import json\n",
    "\n",
    "# ImageDataGenerator를 사용하여 학습 데이터 로드 및 전처리\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'C:/Users/youngjin/Desktop/design',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 클래스 인덱스 확인 및 저장\n",
    "class_indices = train_generator.class_indices\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)\n",
    "\n",
    "print(class_indices)  # 예: {'빵': 0, '케이크': 1, '쿠키': 2, ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5475 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator를 사용하여 이미지 로드 및 전처리\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_directory = 'C:/Users/youngjin/Desktop/accu' # \"빵\" 이미지가 있는 디렉토리 경로\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=(224, 224),  # 모델에 맞는 이미지 크기로 조정\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # 순서를 섞지 않음\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 예측 수행\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predicted_class_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to retrieve element \u001b[39m\u001b[38;5;132;01m{idx}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the Sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas length \u001b[39m\u001b[38;5;132;01m{length}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx\u001b[38;5;241m=\u001b[39midx, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m    107\u001b[0m         )\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[1;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "# 예측 수행\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_class_indices = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.15\n"
     ]
    }
   ],
   "source": [
    "# \"빵\" 클래스 인덱스 (예를 들어, 클래스 인덱스가 0이라고 가정)\n",
    "bread_class_index = class_indices[\"감자\"]\n",
    "\n",
    "# 실제 레이블 생성 (모든 레이블이 \"빵\" 클래스인 경우)\n",
    "true_labels = [bread_class_index] * len(predicted_class_indices)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(true_labels, predicted_class_indices)\n",
    "print(f'정확도: {round(accuracy,4)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4638 images belonging to 1 classes.\n",
      "145/145 [==============================] - 54s 376ms/step\n",
      "Accuracy for 감자: 0.92\n",
      "Found 118 images belonging to 1 classes.\n",
      "4/4 [==============================] - 1s 342ms/step\n",
      "Accuracy for 김치: 0.00\n",
      "Found 551 images belonging to 1 classes.\n",
      "18/18 [==============================] - 7s 366ms/step\n",
      "Accuracy for 달걀: 0.00\n",
      "Found 107 images belonging to 1 classes.\n",
      "4/4 [==============================] - 2s 480ms/step\n",
      "Accuracy for 빵: 0.09\n",
      "Found 14 images belonging to 1 classes.\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "Accuracy for 치즈: 0.00\n",
      "Found 22 images belonging to 1 classes.\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "Accuracy for 양파: 0.09\n",
      "Found 25 images belonging to 1 classes.\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "Accuracy for 파: 0.04\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "class_directories = ['감자', '김치', '달걀',\"빵\",\"치즈\",\"양파\",\"파\"]  # 각 클래스 디렉토리 이름\n",
    "accuracies = []\n",
    "\n",
    "for class_dir in class_directories:\n",
    "    test_directory = f'C:/Users/youngjin/Desktop/accu/{class_dir}'\n",
    "    \n",
    "    test_generator = datagen.flow_from_directory(\n",
    "        test_directory,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 모델 예측\n",
    "    predictions = model.predict(test_generator)\n",
    "    y_true = test_generator.classes\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # 클래스별 정확도 계산\n",
    "    class_accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracies.append(class_accuracy)\n",
    "    print(f\"Accuracy for {class_dir}: {class_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 평균 정확도 계산\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "낮은 이미지 인식률 계선을 위한 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Flatten, DepthwiseConv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전이학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 로드\n",
    "original_model = load_model(\"C:/Users/youngjin/Desktop/converted_keras/keras_model.h5\", \n",
    "                            custom_objects={'DepthwiseConv2D': DepthwiseConv2D}, \n",
    "                            compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 107 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 증강기 설정 (필요에 따라 설정)\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "# 추가할 새로운 데이터 \"빵\"\n",
    "bread_generator = datagen.flow_from_directory(\n",
    "    \"C:/Users/youngjin/Desktop/bread\",\n",
    "    target_size=(224, 224),  # 이미지 크기를 모델에 맞게 조정\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # 다중 클래스 분류\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2972 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 로드\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    \"C:/Users/youngjin/Desktop/design\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_existing_data, y_existing_data = train_generator.next()\n",
    "# 데이터 합치기 용량 부족하여 제너레이터로 나눠서 진행\n",
    "import numpy as np\n",
    "\n",
    "def combined_generator(gen1, gen2):\n",
    "    while True:\n",
    "        X1, y1 = gen1.next()\n",
    "        X2, y2 = gen2.next()\n",
    "        X_combined = np.concatenate([X1, X2], axis=0)\n",
    "        y_combined = np.concatenate([y1, y2], axis=0)\n",
    "        yield X_combined, y_combined\n",
    "\n",
    "# 새로운 훈련 데이터 제너레이터\n",
    "combined_train_generator = combined_generator(train_generator, bread_generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강기 설정 (예시)\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_7 (Sequential)   (None, 1280)              410208    \n",
      "                                                                 \n",
      " sequential_9 (Sequential)   (None, 8)                 128900    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 539108 (2.06 MB)\n",
      "Trainable params: 525028 (2.00 MB)\n",
      "Non-trainable params: 14080 (55.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='model3_input'), name='model3_input', description=\"created by layer 'model3_input'\") at layer \"model3\". The following previous layers were accessed without issue: []. 중간 레이어 이름을 확인해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 중간 레이어 이름 확인\n",
    "layer_name = 'sequential_7'\n",
    "\n",
    "try:\n",
    "    intermediate_layer = original_model.get_layer(layer_name)\n",
    "    intermediate_layer_output = intermediate_layer.output\n",
    "    inputs = original_model.input\n",
    "\n",
    "    # 중간 레이어 모델 생성\n",
    "    intermediate_layer_model = Model(inputs=inputs, outputs=intermediate_layer_output)\n",
    "    intermediate_layer_model.summary()\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}. 중간 레이어 이름을 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이학습을 위한 모델 설정 (기존 모델의 일부 레이어를 고정하고 새로운 출력 레이어 추가)\n",
    "for layer in original_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 출력 레이어 추가\n",
    "x = original_model.layers[0].output  # sequential_7의 출력을 가져옴\n",
    "x = Flatten()(x)\n",
    "new_output = Dense(8, activation='softmax')(x)  # 8개의 클래스를 위한 출력 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='model3_input'), name='model3_input', description=\"created by layer 'model3_input'\") at layer \"model3\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 새로운 모델 정의\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transfer_learning_model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py:166\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    158\u001b[0m         [\n\u001b[0;32m    159\u001b[0m             functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[0;32m    160\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    161\u001b[0m         ]\n\u001b[0;32m    162\u001b[0m     ):\n\u001b[0;32m    163\u001b[0m         inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(\n\u001b[0;32m    164\u001b[0m             inputs, outputs\n\u001b[0;32m    165\u001b[0m         )\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py:265\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_map_graph_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[1;32mc:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py:1145\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39mkeras_inputs):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(x) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m computable_tensors:\n\u001b[1;32m-> 1145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1146\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph disconnected: cannot obtain value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1147\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following previous layers were accessed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout issue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers_with_complete_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1150\u001b[0m         )\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[0;32m   1152\u001b[0m     computable_tensors\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(x))\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='model3_input'), name='model3_input', description=\"created by layer 'model3_input'\") at layer \"model3\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# 새로운 모델 정의\n",
    "transfer_learning_model = Model(inputs=original_model.input, outputs=new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator) + len(bread_generator)\n",
    "transfer_learning_model.fit(combined_train_generator, \n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이학습 모델 저장\n",
    "transfer_learning_model.save(\"path/to/transfer_learning_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인 튜닝을 위한 모델 설정 (모든 레이어를 학습 가능하게 설정)\n",
    "fine_tuning_model = Model(inputs=original_model.input, outputs=new_output)\n",
    "for layer in fine_tuning_model.layers:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강을 사용한 파인 튜닝 모델 학습\n",
    "fine_tuning_model.fit(datagen.flow(X_new_data, y_new_data, batch_size=32),\n",
    "                      epochs=10, \n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인 튜닝 모델 저장\n",
    "fine_tuning_model.save(\"path/to/fine_tuning_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 평가를 위한 테스트 데이터 로드 (예시)\n",
    "X_test, y_test = load_test_dataset(\"path/to/test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "original_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "original_loss, original_accuracy = original_model.evaluate(X_test, y_test)\n",
    "\n",
    "transfer_learning_loss, transfer_learning_accuracy = transfer_learning_model.evaluate(X_test, y_test)\n",
    "\n",
    "fine_tuning_loss, fine_tuning_accuracy = fine_tuning_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Original Model Accuracy: {original_accuracy * 100:.2f}%\")\n",
    "print(f\"Transfer Learning Model Accuracy: {transfer_learning_accuracy * 100:.2f}%\")\n",
    "print(f\"Fine Tuning Model Accuracy: {fine_tuning_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn 모델 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "base_dir = 'C:/Users/youngjin/Desktop/design'\n",
    "\n",
    "#학습 데이터 증대 및 분할 설정\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode =\"nearest\",\n",
    "    validation_split = 0.2 # 전체 데이터의 20%를 검증 데이터로 사용\n",
    ")\n",
    "\n",
    "# 검증 데이터 설정 \n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    validation_split = 0.2 #전체 데이터의 20% 검증 데이터로 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def verify_images(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()  # 이미지 파일이 유효한지 검사\n",
    "            except (IOError, SyntaxError) as e:\n",
    "                print(f\"손상된 이미지 파일 발견: {file_path}\")\n",
    "\n",
    "# 데이터 경로 설정\n",
    "base_dir = 'C:/Users/youngjin/Desktop/design'\n",
    "verify_images(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12136 images belonging to 8 classes.\n",
      "Found 3028 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = \"categorical\",\n",
    "    subset = \"training\" # 학습으로 사용\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size = (224,224),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 아키텍처 설정\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)), #합성곱 층, filter 32, filter size 3x3\n",
    "    MaxPooling2D((2, 2)), #풀링 층, pooling size 2x2 \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu'), \n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(), # 다차원 배열 1차원으로 변환\n",
    "    Dense(512, activation='relu'), #완전 연결층 512node\n",
    "    Dropout(0.5), #드롭아웃 (과적합 방지)\n",
    "    Dense(256, activation='relu'),  \n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint 콜백 설정\n",
    "checkpoint_filepath = 'C:/Users/youngjin/Desktop/CNN_model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',  # 검증 정확도를 기준으로 모델 저장\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.2762 - accuracy: 0.5300\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57680, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 621s 2s/step - loss: 1.2762 - accuracy: 0.5300 - val_loss: 1.0191 - val_accuracy: 0.5768\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.8311 - accuracy: 0.7172\n",
      "Epoch 2: val_accuracy improved from 0.57680 to 0.67287, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.8311 - accuracy: 0.7172 - val_loss: 0.9637 - val_accuracy: 0.6729\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.6790 - accuracy: 0.7673\n",
      "Epoch 3: val_accuracy improved from 0.67287 to 0.70811, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.6790 - accuracy: 0.7673 - val_loss: 1.3078 - val_accuracy: 0.7081\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.7865\n",
      "Epoch 4: val_accuracy did not improve from 0.70811\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.6191 - accuracy: 0.7865 - val_loss: 0.9817 - val_accuracy: 0.6905\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.8138\n",
      "Epoch 5: val_accuracy improved from 0.70811 to 0.74568, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 553s 1s/step - loss: 0.5394 - accuracy: 0.8138 - val_loss: 0.8810 - val_accuracy: 0.7457\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8280\n",
      "Epoch 6: val_accuracy did not improve from 0.74568\n",
      "379/379 [==============================] - 554s 1s/step - loss: 0.5037 - accuracy: 0.8280 - val_loss: 0.9763 - val_accuracy: 0.7370\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.4687 - accuracy: 0.8395\n",
      "Epoch 7: val_accuracy improved from 0.74568 to 0.77128, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 501s 1s/step - loss: 0.4687 - accuracy: 0.8395 - val_loss: 0.9022 - val_accuracy: 0.7713\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.4490 - accuracy: 0.8500\n",
      "Epoch 8: val_accuracy improved from 0.77128 to 0.77194, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 481s 1s/step - loss: 0.4490 - accuracy: 0.8500 - val_loss: 0.8082 - val_accuracy: 0.7719\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.8592\n",
      "Epoch 9: val_accuracy improved from 0.77194 to 0.80552, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 482s 1s/step - loss: 0.4186 - accuracy: 0.8592 - val_loss: 0.7857 - val_accuracy: 0.8055\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8643\n",
      "Epoch 10: val_accuracy did not improve from 0.80552\n",
      "379/379 [==============================] - 484s 1s/step - loss: 0.3959 - accuracy: 0.8643 - val_loss: 1.1294 - val_accuracy: 0.8025\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.8697\n",
      "Epoch 11: val_accuracy improved from 0.80552 to 0.80751, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 483s 1s/step - loss: 0.3832 - accuracy: 0.8697 - val_loss: 1.0042 - val_accuracy: 0.8075\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.8765\n",
      "Epoch 12: val_accuracy did not improve from 0.80751\n",
      "379/379 [==============================] - 482s 1s/step - loss: 0.3656 - accuracy: 0.8765 - val_loss: 0.8338 - val_accuracy: 0.7955\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8778\n",
      "Epoch 13: val_accuracy improved from 0.80751 to 0.81150, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 481s 1s/step - loss: 0.3527 - accuracy: 0.8778 - val_loss: 0.8267 - val_accuracy: 0.8115\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8875\n",
      "Epoch 14: val_accuracy did not improve from 0.81150\n",
      "379/379 [==============================] - 520s 1s/step - loss: 0.3355 - accuracy: 0.8875 - val_loss: 1.0242 - val_accuracy: 0.7743\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.8915\n",
      "Epoch 15: val_accuracy did not improve from 0.81150\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.3240 - accuracy: 0.8915 - val_loss: 0.8176 - val_accuracy: 0.8095\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.8918\n",
      "Epoch 16: val_accuracy did not improve from 0.81150\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.3241 - accuracy: 0.8918 - val_loss: 0.7515 - val_accuracy: 0.8078\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3133 - accuracy: 0.8979\n",
      "Epoch 17: val_accuracy improved from 0.81150 to 0.82912, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 525s 1s/step - loss: 0.3133 - accuracy: 0.8979 - val_loss: 0.6170 - val_accuracy: 0.8291\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8961\n",
      "Epoch 18: val_accuracy did not improve from 0.82912\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.3120 - accuracy: 0.8961 - val_loss: 0.7160 - val_accuracy: 0.8248\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9078\n",
      "Epoch 19: val_accuracy improved from 0.82912 to 0.83245, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 525s 1s/step - loss: 0.2811 - accuracy: 0.9078 - val_loss: 0.7006 - val_accuracy: 0.8324\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9048\n",
      "Epoch 20: val_accuracy did not improve from 0.83245\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.2852 - accuracy: 0.9048 - val_loss: 1.2620 - val_accuracy: 0.7753\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.9019\n",
      "Epoch 21: val_accuracy improved from 0.83245 to 0.83411, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.2919 - accuracy: 0.9019 - val_loss: 0.6308 - val_accuracy: 0.8341\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9160\n",
      "Epoch 22: val_accuracy did not improve from 0.83411\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.2575 - accuracy: 0.9160 - val_loss: 1.1980 - val_accuracy: 0.8148\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9164\n",
      "Epoch 23: val_accuracy did not improve from 0.83411\n",
      "379/379 [==============================] - 522s 1s/step - loss: 0.2550 - accuracy: 0.9164 - val_loss: 0.8670 - val_accuracy: 0.8278\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.9082\n",
      "Epoch 24: val_accuracy did not improve from 0.83411\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.2761 - accuracy: 0.9082 - val_loss: 0.7347 - val_accuracy: 0.8085\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9133\n",
      "Epoch 25: val_accuracy did not improve from 0.83411\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.2534 - accuracy: 0.9133 - val_loss: 0.7584 - val_accuracy: 0.7972\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9165\n",
      "Epoch 26: val_accuracy did not improve from 0.83411\n",
      "379/379 [==============================] - 649s 2s/step - loss: 0.2598 - accuracy: 0.9165 - val_loss: 0.6576 - val_accuracy: 0.8275\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9232\n",
      "Epoch 27: val_accuracy improved from 0.83411 to 0.83810, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 638s 2s/step - loss: 0.2298 - accuracy: 0.9232 - val_loss: 0.5996 - val_accuracy: 0.8381\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.9166\n",
      "Epoch 28: val_accuracy improved from 0.83810 to 0.84009, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 539s 1s/step - loss: 0.2531 - accuracy: 0.9166 - val_loss: 0.7513 - val_accuracy: 0.8401\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.9235\n",
      "Epoch 29: val_accuracy improved from 0.84009 to 0.84641, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 528s 1s/step - loss: 0.2341 - accuracy: 0.9235 - val_loss: 0.6716 - val_accuracy: 0.8464\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.9237\n",
      "Epoch 30: val_accuracy improved from 0.84641 to 0.85938, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 558s 1s/step - loss: 0.2360 - accuracy: 0.9237 - val_loss: 0.5909 - val_accuracy: 0.8594\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9253\n",
      "Epoch 31: val_accuracy improved from 0.85938 to 0.86203, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.2291 - accuracy: 0.9253 - val_loss: 0.5382 - val_accuracy: 0.8620\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2178 - accuracy: 0.9302\n",
      "Epoch 32: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 528s 1s/step - loss: 0.2178 - accuracy: 0.9302 - val_loss: 0.6121 - val_accuracy: 0.8561\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9224\n",
      "Epoch 33: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 532s 1s/step - loss: 0.2426 - accuracy: 0.9224 - val_loss: 0.6817 - val_accuracy: 0.8364\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9269\n",
      "Epoch 34: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 534s 1s/step - loss: 0.2284 - accuracy: 0.9269 - val_loss: 0.6228 - val_accuracy: 0.8547\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9340\n",
      "Epoch 35: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 528s 1s/step - loss: 0.1972 - accuracy: 0.9340 - val_loss: 0.8580 - val_accuracy: 0.8278\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9394\n",
      "Epoch 36: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.1890 - accuracy: 0.9394 - val_loss: 1.0788 - val_accuracy: 0.8059\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9341\n",
      "Epoch 37: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 535s 1s/step - loss: 0.2062 - accuracy: 0.9341 - val_loss: 0.6552 - val_accuracy: 0.8524\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9339\n",
      "Epoch 38: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 530s 1s/step - loss: 0.2048 - accuracy: 0.9339 - val_loss: 0.6038 - val_accuracy: 0.8561\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9387\n",
      "Epoch 39: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 527s 1s/step - loss: 0.1950 - accuracy: 0.9387 - val_loss: 0.7112 - val_accuracy: 0.8394\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9320\n",
      "Epoch 40: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 530s 1s/step - loss: 0.2126 - accuracy: 0.9320 - val_loss: 0.6193 - val_accuracy: 0.8594\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9372\n",
      "Epoch 41: val_accuracy did not improve from 0.86203\n",
      "379/379 [==============================] - 535s 1s/step - loss: 0.1889 - accuracy: 0.9372 - val_loss: 0.6744 - val_accuracy: 0.8531\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9381\n",
      "Epoch 42: val_accuracy improved from 0.86203 to 0.87101, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.1961 - accuracy: 0.9381 - val_loss: 0.6091 - val_accuracy: 0.8710\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.9339\n",
      "Epoch 43: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 533s 1s/step - loss: 0.2036 - accuracy: 0.9339 - val_loss: 0.5128 - val_accuracy: 0.8670\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.9440\n",
      "Epoch 44: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 545s 1s/step - loss: 0.1758 - accuracy: 0.9440 - val_loss: 0.7092 - val_accuracy: 0.8620\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9445\n",
      "Epoch 45: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 541s 1s/step - loss: 0.1733 - accuracy: 0.9445 - val_loss: 0.6226 - val_accuracy: 0.8514\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9408\n",
      "Epoch 46: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 527s 1s/step - loss: 0.1864 - accuracy: 0.9408 - val_loss: 0.5673 - val_accuracy: 0.8630\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9396\n",
      "Epoch 47: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.1950 - accuracy: 0.9396 - val_loss: 0.6753 - val_accuracy: 0.8484\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.9451\n",
      "Epoch 48: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 565s 1s/step - loss: 0.1817 - accuracy: 0.9451 - val_loss: 0.6596 - val_accuracy: 0.8554\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9456\n",
      "Epoch 49: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 537s 1s/step - loss: 0.1652 - accuracy: 0.9456 - val_loss: 0.7591 - val_accuracy: 0.8537\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9399\n",
      "Epoch 50: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 549s 1s/step - loss: 0.1838 - accuracy: 0.9399 - val_loss: 0.7231 - val_accuracy: 0.8541\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9463\n",
      "Epoch 51: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.1679 - accuracy: 0.9463 - val_loss: 0.6115 - val_accuracy: 0.8654\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9475\n",
      "Epoch 52: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.1691 - accuracy: 0.9475 - val_loss: 0.5586 - val_accuracy: 0.8707\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9453\n",
      "Epoch 53: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 527s 1s/step - loss: 0.1744 - accuracy: 0.9453 - val_loss: 1.1692 - val_accuracy: 0.8344\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9415\n",
      "Epoch 54: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 536s 1s/step - loss: 0.1846 - accuracy: 0.9415 - val_loss: 0.7187 - val_accuracy: 0.8504\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9432\n",
      "Epoch 55: val_accuracy did not improve from 0.87101\n",
      "379/379 [==============================] - 531s 1s/step - loss: 0.1820 - accuracy: 0.9432 - val_loss: 0.6800 - val_accuracy: 0.8421\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.9446\n",
      "Epoch 56: val_accuracy improved from 0.87101 to 0.87267, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 527s 1s/step - loss: 0.1728 - accuracy: 0.9446 - val_loss: 0.4678 - val_accuracy: 0.8727\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9428\n",
      "Epoch 57: val_accuracy did not improve from 0.87267\n",
      "379/379 [==============================] - 535s 1s/step - loss: 0.1769 - accuracy: 0.9428 - val_loss: 0.8381 - val_accuracy: 0.8481\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9470\n",
      "Epoch 58: val_accuracy did not improve from 0.87267\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.1723 - accuracy: 0.9470 - val_loss: 0.6324 - val_accuracy: 0.8537\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1636 - accuracy: 0.9458\n",
      "Epoch 59: val_accuracy did not improve from 0.87267\n",
      "379/379 [==============================] - 530s 1s/step - loss: 0.1636 - accuracy: 0.9458 - val_loss: 0.6748 - val_accuracy: 0.8670\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9503\n",
      "Epoch 60: val_accuracy did not improve from 0.87267\n",
      "379/379 [==============================] - 528s 1s/step - loss: 0.1610 - accuracy: 0.9503 - val_loss: 0.5424 - val_accuracy: 0.8693\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9500\n",
      "Epoch 61: val_accuracy improved from 0.87267 to 0.87566, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 536s 1s/step - loss: 0.1558 - accuracy: 0.9500 - val_loss: 0.6413 - val_accuracy: 0.8757\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.9551\n",
      "Epoch 62: val_accuracy did not improve from 0.87566\n",
      "379/379 [==============================] - 529s 1s/step - loss: 0.1394 - accuracy: 0.9551 - val_loss: 0.6702 - val_accuracy: 0.8723\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 0.9483\n",
      "Epoch 63: val_accuracy did not improve from 0.87566\n",
      "379/379 [==============================] - 528s 1s/step - loss: 0.1805 - accuracy: 0.9483 - val_loss: 0.5707 - val_accuracy: 0.8394\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9518\n",
      "Epoch 64: val_accuracy improved from 0.87566 to 0.87600, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 536s 1s/step - loss: 0.1491 - accuracy: 0.9518 - val_loss: 0.5847 - val_accuracy: 0.8760\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.9442\n",
      "Epoch 65: val_accuracy did not improve from 0.87600\n",
      "379/379 [==============================] - 533s 1s/step - loss: 0.1728 - accuracy: 0.9442 - val_loss: 0.6906 - val_accuracy: 0.8447\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9491\n",
      "Epoch 66: val_accuracy did not improve from 0.87600\n",
      "379/379 [==============================] - 540s 1s/step - loss: 0.1657 - accuracy: 0.9491 - val_loss: 0.7846 - val_accuracy: 0.8421\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9549\n",
      "Epoch 67: val_accuracy improved from 0.87600 to 0.88231, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 540s 1s/step - loss: 0.1453 - accuracy: 0.9549 - val_loss: 0.5596 - val_accuracy: 0.8823\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9527\n",
      "Epoch 68: val_accuracy did not improve from 0.88231\n",
      "379/379 [==============================] - 541s 1s/step - loss: 0.1511 - accuracy: 0.9527 - val_loss: 0.8499 - val_accuracy: 0.8521\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9535\n",
      "Epoch 69: val_accuracy did not improve from 0.88231\n",
      "379/379 [==============================] - 534s 1s/step - loss: 0.1623 - accuracy: 0.9535 - val_loss: 0.5756 - val_accuracy: 0.8624\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9480\n",
      "Epoch 70: val_accuracy did not improve from 0.88231\n",
      "379/379 [==============================] - 550s 1s/step - loss: 0.1665 - accuracy: 0.9480 - val_loss: 0.5896 - val_accuracy: 0.8690\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9588\n",
      "Epoch 71: val_accuracy did not improve from 0.88231\n",
      "379/379 [==============================] - 532s 1s/step - loss: 0.1316 - accuracy: 0.9588 - val_loss: 0.5373 - val_accuracy: 0.8743\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9556\n",
      "Epoch 72: val_accuracy did not improve from 0.88231\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.1397 - accuracy: 0.9556 - val_loss: 0.7066 - val_accuracy: 0.8597\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9528\n",
      "Epoch 73: val_accuracy did not improve from 0.88231\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.1533 - accuracy: 0.9528 - val_loss: 0.7085 - val_accuracy: 0.8700\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9615\n",
      "Epoch 74: val_accuracy improved from 0.88231 to 0.88730, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 549s 1s/step - loss: 0.1276 - accuracy: 0.9615 - val_loss: 0.5555 - val_accuracy: 0.8873\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9578\n",
      "Epoch 75: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 527s 1s/step - loss: 0.1296 - accuracy: 0.9578 - val_loss: 0.7552 - val_accuracy: 0.8384\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9556\n",
      "Epoch 76: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.1423 - accuracy: 0.9556 - val_loss: 0.6573 - val_accuracy: 0.8667\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9448\n",
      "Epoch 77: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.1830 - accuracy: 0.9448 - val_loss: 0.6389 - val_accuracy: 0.8617\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9570\n",
      "Epoch 78: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 525s 1s/step - loss: 0.1433 - accuracy: 0.9570 - val_loss: 0.8588 - val_accuracy: 0.8590\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9563\n",
      "Epoch 79: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 522s 1s/step - loss: 0.1469 - accuracy: 0.9563 - val_loss: 0.7028 - val_accuracy: 0.8610\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9568\n",
      "Epoch 80: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 522s 1s/step - loss: 0.1436 - accuracy: 0.9568 - val_loss: 0.6293 - val_accuracy: 0.8727\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9619\n",
      "Epoch 81: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 527s 1s/step - loss: 0.1170 - accuracy: 0.9619 - val_loss: 0.5655 - val_accuracy: 0.8797\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9550\n",
      "Epoch 82: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 522s 1s/step - loss: 0.1557 - accuracy: 0.9550 - val_loss: 0.6434 - val_accuracy: 0.8537\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9513\n",
      "Epoch 83: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.1536 - accuracy: 0.9513 - val_loss: 0.6626 - val_accuracy: 0.8770\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9613\n",
      "Epoch 84: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.1229 - accuracy: 0.9613 - val_loss: 0.8187 - val_accuracy: 0.8464\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1450 - accuracy: 0.9561\n",
      "Epoch 85: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.1450 - accuracy: 0.9561 - val_loss: 0.6272 - val_accuracy: 0.8647\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9557\n",
      "Epoch 86: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.1480 - accuracy: 0.9557 - val_loss: 0.6532 - val_accuracy: 0.8740\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.9517\n",
      "Epoch 87: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 521s 1s/step - loss: 0.1589 - accuracy: 0.9517 - val_loss: 0.7578 - val_accuracy: 0.8584\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9580\n",
      "Epoch 88: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.1361 - accuracy: 0.9580 - val_loss: 0.7282 - val_accuracy: 0.8514\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9563\n",
      "Epoch 89: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 522s 1s/step - loss: 0.1398 - accuracy: 0.9563 - val_loss: 0.9180 - val_accuracy: 0.8311\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9608\n",
      "Epoch 90: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.1293 - accuracy: 0.9608 - val_loss: 0.7850 - val_accuracy: 0.8524\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9559\n",
      "Epoch 91: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 538s 1s/step - loss: 0.1508 - accuracy: 0.9559 - val_loss: 0.6857 - val_accuracy: 0.8710\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9616\n",
      "Epoch 92: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 527s 1s/step - loss: 0.1193 - accuracy: 0.9616 - val_loss: 0.8198 - val_accuracy: 0.8667\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9600\n",
      "Epoch 93: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.1351 - accuracy: 0.9600 - val_loss: 0.9102 - val_accuracy: 0.8421\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9606\n",
      "Epoch 94: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 523s 1s/step - loss: 0.1358 - accuracy: 0.9606 - val_loss: 0.7260 - val_accuracy: 0.8654\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9604\n",
      "Epoch 95: val_accuracy did not improve from 0.88730\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.1420 - accuracy: 0.9604 - val_loss: 0.6826 - val_accuracy: 0.8733\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9628\n",
      "Epoch 96: val_accuracy improved from 0.88730 to 0.89195, saving model to C:/Users/youngjin/Desktop\\CNN_model.h5\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.1233 - accuracy: 0.9628 - val_loss: 0.6003 - val_accuracy: 0.8920\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.9568\n",
      "Epoch 97: val_accuracy did not improve from 0.89195\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.1465 - accuracy: 0.9568 - val_loss: 0.9872 - val_accuracy: 0.8414\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9600\n",
      "Epoch 98: val_accuracy did not improve from 0.89195\n",
      "379/379 [==============================] - 524s 1s/step - loss: 0.1373 - accuracy: 0.9600 - val_loss: 0.8923 - val_accuracy: 0.8767\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9588\n",
      "Epoch 99: val_accuracy did not improve from 0.89195\n",
      "379/379 [==============================] - 526s 1s/step - loss: 0.1352 - accuracy: 0.9588 - val_loss: 0.9646 - val_accuracy: 0.8783\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9594\n",
      "Epoch 100: val_accuracy did not improve from 0.89195\n",
      "379/379 [==============================] - 525s 1s/step - loss: 0.1268 - accuracy: 0.9594 - val_loss: 1.0940 - val_accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:/Users/youngjin/Desktop/CNN_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.layers import DepthwiseConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\youngjin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CNN_model_1 = load_model(\"C:/Users/youngjin/Desktop/CNN_model_1.h5\",\n",
    "                         custom_objects= {\"DepthwiseConv2D\":DepthwiseConv2D},\n",
    "                         compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 12, 12, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 36864)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               18874880  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19396680 (73.99 MB)\n",
      "Trainable params: 19396680 (73.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_model_1.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
